[05/05 12:42:08] train INFO: Namespace(fp32_resume=False, batch_size=128, epochs=310, config='config/simba_s.py', model='simba_s', input_size=224, drop=0.0, drop_path=0.1, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='dataset/ILSVRC2012', data_set='IMNET', use_mcloader=False, inat_category='name', output_dir='checkpoints/simba_s', device='cuda', seed=0, resume='', start_epoch=0, eval=False, dist_eval=False, num_workers=24, pin_mem=True, world_size=2, dist_url='env://', token_label=True, token_label_data='', token_label_size=1, dense_weight=0.5, cls_weight=1.0, no_aug=False, scale=[0.08, 1.0], ratio=[0.75, 1.3333333333333333], hflip=0.5, vflip=0.0, use_multi_epochs_loader=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[05/05 12:42:09] train INFO: Creating model: simba_s
[05/05 12:42:16] train INFO: number of params: 26591088
[05/05 12:42:16] train INFO: Start training for 310 epochs
[05/05 12:49:56] train INFO: Namespace(fp32_resume=False, batch_size=128, epochs=310, config='config/simba_s.py', model='simba_s', input_size=224, drop=0.0, drop_path=0.1, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.001, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='dataset/ILSVRC2012', data_set='IMNET', use_mcloader=False, inat_category='name', output_dir='checkpoints/simba_s', device='cuda', seed=0, resume='', start_epoch=0, eval=False, dist_eval=False, num_workers=24, pin_mem=True, world_size=2, dist_url='env://', token_label=True, token_label_data='', token_label_size=1, dense_weight=0.5, cls_weight=1.0, no_aug=False, scale=[0.08, 1.0], ratio=[0.75, 1.3333333333333333], hflip=0.5, vflip=0.0, use_multi_epochs_loader=False, rank=1, gpu=1, distributed=True, dist_backend='nccl')
[05/05 12:49:57] train INFO: Creating model: simba_s
[05/05 12:50:04] train INFO: number of params: 26591088
[05/05 12:50:04] train INFO: Start training for 310 epochs
