Running
Sun May 11 11:12:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.51.03              Driver Version: 575.51.03      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 NVL                Off |   00000000:2A:00.0 Off |                    0 |
| N/A   42C    P0             99W /  400W |   16126MiB /  95830MiB |    100%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 NVL                Off |   00000000:99:00.0 Off |                    0 |
| N/A   35C    P0             59W /  400W |      12MiB /  95830MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A         1017815      C   python                                 3000MiB |
|    0   N/A  N/A         1058310      C   python                                 3066MiB |
|    0   N/A  N/A         1209015      C   python                                 7330MiB |
|    0   N/A  N/A         1209684      C   python                                 1412MiB |
|    0   N/A  N/A         1212178      C   python                                 1282MiB |
+-----------------------------------------------------------------------------------------+

Resuming from checkpoint: checkpoints/simba_l/checkpoint-29.pth.tar
/volume1/users/rgeens/simba/env/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.
  warnings.warn(
| distributed init (rank 0): env://
[rank0]:[W511 11:12:26.304059791 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
wandb: Currently logged in as: rgeens (kuleuven-micas) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /volume1/users/rgeens/simba/wandb/run-20250511_111227-rel2zngg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run simba_20250511_111227
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kuleuven-micas/simba
wandb: üöÄ View run at https://wandb.ai/kuleuven-micas/simba/runs/rel2zngg
[05/11 11:12:28] train INFO: Namespace(fp32_resume=False, batch_size=256, epochs=310, config='config/simba_l.py', model='simba_l', input_size=224, drop=0.0, drop_path=0.3, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=1.0, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.002, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', data_path='dataset/ILSVRC2012', data_set='IMNET', use_mcloader=False, inat_category='name', output_dir='checkpoints/simba_l', device='cuda', seed=0, resume='checkpoints/simba_l/checkpoint-29.pth.tar', start_epoch=0, eval=False, dist_eval=False, num_workers=12, pin_mem=True, world_size=1, dist_url='env://', token_label=False, token_label_data='', token_label_size=1, dense_weight=0.5, cls_weight=1.0, no_aug=False, scale=[0.08, 1.0], ratio=[0.75, 1.3333333333333333], hflip=0.5, vflip=0.0, use_multi_epochs_loader=False, rank=0, gpu=0, distributed=True, dist_backend='nccl')
[05/11 11:12:29] train INFO: Creating model: simba_l
SiMBA(
  (patch_embed1): Stem(
    (conv): Sequential(
      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU(inplace=True)
      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (8): ReLU(inplace=True)
    )
    (proj): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block_mamba(
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): MambaLayer(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=96, out_features=384, bias=False)
          (conv1d): Conv1d(192, 192, kernel_size=(4,), stride=(1,), padding=(3,), groups=192)
          (act): SiLU()
          (x_proj): Linear(in_features=192, out_features=134, bias=False)
          (dt_proj): Linear(in_features=6, out_features=192, bias=True)
          (out_proj): Linear(in_features=192, out_features=96, bias=False)
        )
      )
      (mlp): EinFFT()
      (drop_path): Identity()
    )
    (1-2): 2 x Block_mamba(
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): MambaLayer(
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=96, out_features=384, bias=False)
          (conv1d): Conv1d(192, 192, kernel_size=(4,), stride=(1,), padding=(3,), groups=192)
          (act): SiLU()
          (x_proj): Linear(in_features=192, out_features=134, bias=False)
          (dt_proj): Linear(in_features=6, out_features=192, bias=True)
          (out_proj): Linear(in_features=192, out_features=96, bias=False)
        )
      )
      (mlp): EinFFT()
      (drop_path): DropPath()
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): DownSamples(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0-5): 6 x Block_mamba(
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): MambaLayer(
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=192, out_features=768, bias=False)
          (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)
          (act): SiLU()
          (x_proj): Linear(in_features=384, out_features=140, bias=False)
          (dt_proj): Linear(in_features=12, out_features=384, bias=True)
          (out_proj): Linear(in_features=384, out_features=192, bias=False)
        )
      )
      (mlp): EinFFT()
      (drop_path): DropPath()
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): DownSamples(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0-17): 18 x Block_mamba(
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MambaLayer(
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=384, out_features=1536, bias=False)
          (conv1d): Conv1d(768, 768, kernel_size=(4,), stride=(1,), padding=(3,), groups=768)
          (act): SiLU()
          (x_proj): Linear(in_features=768, out_features=152, bias=False)
          (dt_proj): Linear(in_features=24, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=384, bias=False)
        )
      )
      (mlp): EinFFT()
      (drop_path): DropPath()
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): DownSamples(
    (proj): Conv2d(384, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0-2): 3 x Block_mamba(
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): MambaLayer(
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=512, out_features=2048, bias=False)
          (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
          (act): SiLU()
          (x_proj): Linear(in_features=1024, out_features=160, bias=False)
          (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=512, bias=False)
        )
      )
      (mlp): EinFFT()
      (drop_path): DropPath()
    )
  )
  (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (post_network): ModuleList(
    (0): ClassBlock(
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): MambaLayer(
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mamba): Mamba(
          (in_proj): Linear(in_features=512, out_features=2048, bias=False)
          (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)
          (act): SiLU()
          (x_proj): Linear(in_features=1024, out_features=160, bias=False)
          (dt_proj): Linear(in_features=32, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=512, bias=False)
        )
      )
      (mlp): EinFFT()
    )
  )
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
/volume1/users/rgeens/simba/simba/simba.py:90: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /pytorch/aten/src/ATen/native/Copy.cpp:307.)
  x = x.to(torch.float32)
Model simba_l created, flops_count: 1.57 GMac, param count: 36.13 M
[05/11 11:12:30] train INFO: number of params: 36128264
/volume1/users/rgeens/simba/env/lib/python3.11/site-packages/timm/utils/cuda.py:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
[05/11 11:12:30] train INFO: <All keys matched successfully>
[05/11 11:12:30] train INFO: Start training for 310 epochs
Start training for extra_epoch20 epochs
[05/11 11:12:36] train INFO: Epoch: [30]  [   0/5004]  eta: 8:35:56  lr: 0.000979  loss: 5.0658 (5.0658)  time: 6.1863  data: 2.7603  max mem: 52608
[05/11 11:12:57] train INFO: Epoch: [30]  [  10/5004]  eta: 3:19:39  lr: 0.000979  loss: 4.9658 (4.7530)  time: 2.3988  data: 0.2510  max mem: 52718
[05/11 11:13:17] train INFO: Epoch: [30]  [  20/5004]  eta: 3:04:41  lr: 0.000979  loss: 4.7042 (4.6059)  time: 2.0253  data: 0.0001  max mem: 52718
[05/11 11:13:37] train INFO: Epoch: [30]  [  30/5004]  eta: 2:59:21  lr: 0.000979  loss: 4.6100 (4.6201)  time: 2.0343  data: 0.0001  max mem: 52718
[05/11 11:13:58] train INFO: Epoch: [30]  [  40/5004]  eta: 2:56:37  lr: 0.000979  loss: 4.4669 (4.5565)  time: 2.0418  data: 0.0002  max mem: 52718
[05/11 11:14:18] train INFO: Epoch: [30]  [  50/5004]  eta: 2:54:51  lr: 0.000979  loss: 4.3877 (4.5373)  time: 2.0467  data: 0.0001  max mem: 52718
[05/11 11:14:39] train INFO: Epoch: [30]  [  60/5004]  eta: 2:53:34  lr: 0.000979  loss: 4.3243 (4.4994)  time: 2.0483  data: 0.0001  max mem: 52718
[05/11 11:14:59] train INFO: Epoch: [30]  [  70/5004]  eta: 2:52:33  lr: 0.000979  loss: 4.3095 (4.4640)  time: 2.0489  data: 0.0001  max mem: 52718
[05/11 11:15:20] train INFO: Epoch: [30]  [  80/5004]  eta: 2:51:41  lr: 0.000979  loss: 4.4100 (4.4453)  time: 2.0485  data: 0.0001  max mem: 52718
[05/11 11:15:40] train INFO: Epoch: [30]  [  90/5004]  eta: 2:50:56  lr: 0.000979  loss: 4.3538 (4.4315)  time: 2.0475  data: 0.0001  max mem: 52718
[05/11 11:16:01] train INFO: Epoch: [30]  [ 100/5004]  eta: 2:50:17  lr: 0.000979  loss: 4.5655 (4.4448)  time: 2.0481  data: 0.0001  max mem: 52718
[05/11 11:16:21] train INFO: Epoch: [30]  [ 110/5004]  eta: 2:49:40  lr: 0.000979  loss: 4.6036 (4.4439)  time: 2.0489  data: 0.0001  max mem: 52718
[05/11 11:16:42] train INFO: Epoch: [30]  [ 120/5004]  eta: 2:49:07  lr: 0.000979  loss: 4.6036 (4.4482)  time: 2.0483  data: 0.0001  max mem: 52718
[05/11 11:17:02] train INFO: Epoch: [30]  [ 130/5004]  eta: 2:48:35  lr: 0.000979  loss: 4.5398 (4.4369)  time: 2.0485  data: 0.0001  max mem: 52718
[05/11 11:17:23] train INFO: Epoch: [30]  [ 140/5004]  eta: 2:48:05  lr: 0.000979  loss: 4.3652 (4.4285)  time: 2.0486  data: 0.0001  max mem: 52718
[05/11 11:17:43] train INFO: Epoch: [30]  [ 150/5004]  eta: 2:47:36  lr: 0.000979  loss: 4.3310 (4.4169)  time: 2.0480  data: 0.0001  max mem: 52718
[05/11 11:18:03] train INFO: Epoch: [30]  [ 160/5004]  eta: 2:47:08  lr: 0.000979  loss: 4.4395 (4.4248)  time: 2.0478  data: 0.0001  max mem: 52718
[05/11 11:18:24] train INFO: Epoch: [30]  [ 170/5004]  eta: 2:46:41  lr: 0.000979  loss: 4.4395 (4.4158)  time: 2.0481  data: 0.0001  max mem: 52718
[05/11 11:18:44] train INFO: Epoch: [30]  [ 180/5004]  eta: 2:46:15  lr: 0.000979  loss: 4.2170 (4.4039)  time: 2.0483  data: 0.0001  max mem: 52718
[05/11 11:19:05] train INFO: Epoch: [30]  [ 190/5004]  eta: 2:45:50  lr: 0.000979  loss: 4.2133 (4.4042)  time: 2.0487  data: 0.0001  max mem: 52718
[05/11 11:19:25] train INFO: Epoch: [30]  [ 200/5004]  eta: 2:45:25  lr: 0.000979  loss: 4.2133 (4.3872)  time: 2.0493  data: 0.0001  max mem: 52718
[05/11 11:19:46] train INFO: Epoch: [30]  [ 210/5004]  eta: 2:45:00  lr: 0.000979  loss: 4.3110 (4.3806)  time: 2.0489  data: 0.0001  max mem: 52718
[05/11 11:20:06] train INFO: Epoch: [30]  [ 220/5004]  eta: 2:44:36  lr: 0.000979  loss: 4.4119 (4.3739)  time: 2.0483  data: 0.0001  max mem: 52718
[05/11 11:20:27] train INFO: Epoch: [30]  [ 230/5004]  eta: 2:44:12  lr: 0.000979  loss: 4.3986 (4.3701)  time: 2.0482  data: 0.0001  max mem: 52718
[05/11 11:20:47] train INFO: Epoch: [30]  [ 240/5004]  eta: 2:43:48  lr: 0.000979  loss: 4.3989 (4.3688)  time: 2.0489  data: 0.0001  max mem: 52718
[05/11 11:21:08] train INFO: Epoch: [30]  [ 250/5004]  eta: 2:43:25  lr: 0.000979  loss: 4.1713 (4.3579)  time: 2.0497  data: 0.0001  max mem: 52718
[05/11 11:21:28] train INFO: Epoch: [30]  [ 260/5004]  eta: 2:43:02  lr: 0.000979  loss: 4.0381 (4.3486)  time: 2.0492  data: 0.0001  max mem: 52718
[05/11 11:21:49] train INFO: Epoch: [30]  [ 270/5004]  eta: 2:42:39  lr: 0.000979  loss: 4.3081 (4.3461)  time: 2.0483  data: 0.0001  max mem: 52718
[05/11 11:22:09] train INFO: Epoch: [30]  [ 280/5004]  eta: 2:42:16  lr: 0.000979  loss: 4.4061 (4.3514)  time: 2.0480  data: 0.0001  max mem: 52718
[05/11 11:22:30] train INFO: Epoch: [30]  [ 290/5004]  eta: 2:41:53  lr: 0.000979  loss: 4.3577 (4.3438)  time: 2.0479  data: 0.0001  max mem: 52718
[05/11 11:22:50] train INFO: Epoch: [30]  [ 300/5004]  eta: 2:41:31  lr: 0.000979  loss: 3.8876 (4.3321)  time: 2.0479  data: 0.0001  max mem: 52718
[05/11 11:23:11] train INFO: Epoch: [30]  [ 310/5004]  eta: 2:41:08  lr: 0.000979  loss: 4.0894 (4.3310)  time: 2.0486  data: 0.0001  max mem: 52718
[05/11 11:23:31] train INFO: Epoch: [30]  [ 320/5004]  eta: 2:40:46  lr: 0.000979  loss: 4.6467 (4.3435)  time: 2.0495  data: 0.0001  max mem: 52718
[05/11 11:23:52] train INFO: Epoch: [30]  [ 330/5004]  eta: 2:40:24  lr: 0.000979  loss: 4.6650 (4.3418)  time: 2.0495  data: 0.0001  max mem: 52718
[05/11 11:24:12] train INFO: Epoch: [30]  [ 340/5004]  eta: 2:40:02  lr: 0.000979  loss: 4.7060 (4.3485)  time: 2.0484  data: 0.0001  max mem: 52718
[05/11 11:24:33] train INFO: Epoch: [30]  [ 350/5004]  eta: 2:39:40  lr: 0.000979  loss: 4.6021 (4.3501)  time: 2.0483  data: 0.0001  max mem: 52718
[05/11 11:24:53] train INFO: Epoch: [30]  [ 360/5004]  eta: 2:39:18  lr: 0.000979  loss: 4.2681 (4.3489)  time: 2.0490  data: 0.0001  max mem: 52718
[05/11 11:25:14] train INFO: Epoch: [30]  [ 370/5004]  eta: 2:38:57  lr: 0.000979  loss: 4.2681 (4.3464)  time: 2.0493  data: 0.0001  max mem: 52718
[05/11 11:25:34] train INFO: Epoch: [30]  [ 380/5004]  eta: 2:38:35  lr: 0.000979  loss: 4.4960 (4.3444)  time: 2.0493  data: 0.0001  max mem: 52718
[05/11 11:25:55] train INFO: Epoch: [30]  [ 390/5004]  eta: 2:38:13  lr: 0.000979  loss: 4.4960 (4.3398)  time: 2.0485  data: 0.0001  max mem: 52718
[05/11 11:26:15] train INFO: Epoch: [30]  [ 400/5004]  eta: 2:37:52  lr: 0.000979  loss: 4.5205 (4.3445)  time: 2.0481  data: 0.0001  max mem: 52718
[05/11 11:26:36] train INFO: Epoch: [30]  [ 410/5004]  eta: 2:37:30  lr: 0.000979  loss: 4.5205 (4.3481)  time: 2.0484  data: 0.0001  max mem: 52718
[05/11 11:26:56] train INFO: Epoch: [30]  [ 420/5004]  eta: 2:37:08  lr: 0.000979  loss: 4.4519 (4.3466)  time: 2.0483  data: 0.0001  max mem: 52718
[05/11 11:27:17] train INFO: Epoch: [30]  [ 430/5004]  eta: 2:36:47  lr: 0.000979  loss: 4.6610 (4.3552)  time: 2.0481  data: 0.0001  max mem: 52718
